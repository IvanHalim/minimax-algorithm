{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Othello\n",
    "\n",
    "Othello is a turn-based two-player strategy board game. The players take turns placing pieces, one player white and the other player black, on an 8x8 board in such a way that captures some of the opponent's pieces, with the goal of finishing the game with more pieces of their color on the board.\n",
    "\n",
    "Every move must capture one or more of the opponent's pieces. To capture, player A places a piece adjacent to one of player B's pieces so that there is a straight line (horizontal, vertical or diagonal) of adjacent pieces that begins with one of player A's pieces, continues with one or more of player B's pieces, and ends with one of player A's pieces.\n",
    "\n",
    "For example, if Black places a piece on square (5, 1), he will capture all of Black's pieces (5, 1) and (5, 6):\n",
    "\n",
    "<img src='files/img/capture.png'>\n",
    "\n",
    "For more information about the game (which is also known as Reversi) including detailed rules, see the entry on [Wikipedia](https://en.wikipedia.org/wiki/Reversi). Additionally, this implementation doesn't take into account some tournament-style Othello details, such as game time limits and a different indexing scheme.\n",
    "\n",
    "We will implement representations for the board and pieces and the mechanics of playing a game. We will then explore several game-playing strategies. There is a simple command-line program provided for playing against the computer or comparing two strategies.\n",
    "\n",
    "Written by [Daniel Connelly](http://dhconnelly.com). This implementation follows chapter 18 of Peter Norvig's \"Paradigms of Artificial Intelligence\".\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Board representation](#board)\n",
    "2. [Playing the game](#playing)\n",
    "3. [Strategies](#strategies)\n",
    "   - [Random](#random)<br>\n",
    "   - [Local maximization](#localmax)<br>\n",
    "   - [Minimax search](#minimax)<br>\n",
    "   - [Alpha-beta search](#alphabeta)<br>\n",
    "   - [Alpha-beta v2.0](#alphabeta2)<br>\n",
    "   - [Alpha-beta v3.0](#alphabeta3)<br>\n",
    "4. [Championship Programs](#championship)<br>\n",
    "   - [Mobility](#mobility)<br>\n",
    "   - [Edge Stability](#edgestability)<br>\n",
    "5. [Other Techniques](#other)<br>\n",
    "6. [Conclusion](#conclusion)\n",
    "\n",
    "<a id=\"board\"></a>\n",
    "\n",
    "## Board Representation\n",
    "\n",
    "We represent the board as a 100-element list, which includes each square on the board as well as the outside edge. Each consecutive sublist of ten elements represents a single row, and each list element stores a piece. An initial board contains four pieces in the center:\n",
    "\n",
    "```\n",
    "     ? ? ? ? ? ? ? ? ? ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . o @ . . . ?\n",
    "     ? . . . @ o . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? ? ? ? ? ? ? ? ? ?\n",
    "```\n",
    "\n",
    "This representation has two useful properties:\n",
    "1. Square (m, n) can be accessed as `board[mn]`. This avoids the need to write functions that convert between square locations and list indexes.\n",
    "2. Operations involving bounds checking are slightly simpler.\n",
    "\n",
    "The outside edge is marked `?`, empty squares are `.`, black is `@`, and white is `o`. The black and white pieces represent the two players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY, BLACK, WHITE, OUTER = '.', '@', 'o', '?'\n",
    "PIECES = (EMPTY, BLACK, WHITE, OUTER)\n",
    "PLAYERS = {BLACK: 'Black', WHITE: 'White'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refer to neighbor squares we can add a direction to a square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP, DOWN, LEFT, RIGHT = -10, 10, -1, 1\n",
    "UP_RIGHT, DOWN_RIGHT, DOWN_LEFT, UP_LEFT = -9, 11, 9, -11\n",
    "DIRECTIONS = (UP, UP_RIGHT, RIGHT, DOWN_RIGHT, DOWN, DOWN_LEFT, LEFT, UP_LEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 2 3 4 5 6 7 8\n",
      "1 . . . . . . . .\n",
      "2 . . . . . . . .\n",
      "3 . . . . . . . .\n",
      "4 . . . o @ . . .\n",
      "5 . . . @ o . . .\n",
      "6 . . . . . . . .\n",
      "7 . . . . . . . .\n",
      "8 . . . . . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def squares():\n",
    "    \"\"\"List all the valid squares on the board\"\"\"\n",
    "    return [i for i in range(11, 89) if 1 <= (i % 10) <= 8]\n",
    "\n",
    "def initial_board():\n",
    "    \"\"\"Create a new board with the initial black and white positions filled\"\"\"\n",
    "    board = [OUTER] * 100\n",
    "    for i in squares():\n",
    "        board[i] = EMPTY\n",
    "    # The middle four squares should hold the initial piece positions.\n",
    "    board[44], board[45] = WHITE, BLACK\n",
    "    board[54], board[55] = BLACK, WHITE\n",
    "    return board\n",
    "\n",
    "def print_board(board):\n",
    "    \"\"\"Get a string representation of the board.\"\"\"\n",
    "    rep = ''\n",
    "    rep += '  {0}\\n'.format(' '.join(map(str, range(1, 9))))\n",
    "    for row in range(1, 9):\n",
    "        begin, end = 10*row + 1, 10*row + 9\n",
    "        rep += '{0} {1}\\n'.format(row, ' '.join(board[begin:end]))\n",
    "    return rep\n",
    "\n",
    "print(print_board(initial_board()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"playing\"></a>\n",
    "\n",
    "## Playing the Game\n",
    "\n",
    "We need functions to get moves from players, check to make sure that the moves are legal, apply the moves to the board, and detect when the game is over.\n",
    "\n",
    "### Checking Moves\n",
    "\n",
    "A move must be both valid and legal: it must refer to a real square, and it must form a bracket with another piece of the same color with pieces of the opposite color in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(move):\n",
    "    \"\"\"Is move a square on the board?\"\"\"\n",
    "    return isinstance(move, int) and move in squares()\n",
    "\n",
    "def opponent(player):\n",
    "    \"\"\"Get player's opponent piece.\"\"\"\n",
    "    return BLACK if player is WHITE else WHITE\n",
    "\n",
    "def find_bracket(square, player, board, direction):\n",
    "    \"\"\"\n",
    "    Find a square that forms a bracket with `square` for `player` in the given\n",
    "    `direction`. Returns None if no such squares exists.\n",
    "    \"\"\"\n",
    "    bracket = square + direction\n",
    "    if board[bracket] == player:\n",
    "        return None\n",
    "    opp = opponent(player)\n",
    "    while board[bracket] == opp:\n",
    "        bracket += direction\n",
    "    return None if board[bracket] in (OUTER, EMPTY) else bracket\n",
    "\n",
    "def is_legal(move, player, board):\n",
    "    \"\"\"Is this a legal move for the player?\"\"\"\n",
    "    hasbracket = lambda direction: find_bracket(move, player, board, direction)\n",
    "    return board[move] == EMPTY and any(map(hasbracket, DIRECTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Moves\n",
    "\n",
    "When the player makes a move, we need to update the board and flip all the bracketed pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_move(move, player, board):\n",
    "    \"\"\"Update the board to reflect the move by the specified player.\"\"\"\n",
    "    board[move] = player\n",
    "    for d in DIRECTIONS:\n",
    "        make_flips(move, player, board, d)\n",
    "    return board\n",
    "\n",
    "def make_flips(move, player, board, direction):\n",
    "    \"\"\"Flip pieces in the given direction as a result of the move by player.\"\"\"\n",
    "    bracket = find_bracket(move, player, board, direction)\n",
    "    if not bracket:\n",
    "        return\n",
    "    square = move + direction\n",
    "    while square != bracket:\n",
    "        board[square] = player\n",
    "        square += direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IllegalMoveError(Exception):\n",
    "    def __init__(self, player, move, board):\n",
    "        self.player = player\n",
    "        self.move = move\n",
    "        self.board = board\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '{0} cannot move to square {1}'.format(PLAYERS[self.player], self.move)\n",
    "    \n",
    "def legal_moves(player, board):\n",
    "    \"\"\"Get a list of all legal moves for player.\"\"\"\n",
    "    return [sq for sq in squares() if is_legal(sq, player, board)]\n",
    "\n",
    "def any_legal_move(player, board):\n",
    "    \"\"\"Can player make any moves?\"\"\"\n",
    "    return any(is_legal(sq, player, board) for sq in squares())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Each round consists of:\n",
    "-  Get a move from the current player.\n",
    "-  Apply it to the board.\n",
    "-  Switch players. If the game is over, get the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_number = 1 # The number of the move to be played\n",
    "\n",
    "def play(black_strategy, white_strategy):\n",
    "    \"\"\"Play a game of Othello and return the final board and score.\"\"\"\n",
    "    board = initial_board()\n",
    "    player = BLACK\n",
    "    strategy = lambda who: black_strategy if who == BLACK else white_strategy\n",
    "    global move_number\n",
    "    move_number = 1\n",
    "    while player is not None:\n",
    "        move = get_move(strategy(player), player, board)\n",
    "        make_move(move, player, board)\n",
    "        player = next_player(board, player)\n",
    "        move_number += 1\n",
    "    return board, score(BLACK, board)\n",
    "\n",
    "def next_player(board, prev_player):\n",
    "    \"\"\"Which player should move next? Returns None if no legal moves exist.\"\"\"\n",
    "    opp = opponent(prev_player)\n",
    "    if any_legal_move(opp, board):\n",
    "        return opp\n",
    "    elif any_legal_move(prev_player, board):\n",
    "        return prev_player\n",
    "    return None\n",
    "\n",
    "def get_move(strategy, player, board):\n",
    "    \"\"\"Call strategy(player, board) to get a move.\"\"\"\n",
    "    copy = board[:] # copy the board to prevent cheating\n",
    "    move = strategy(player, copy)\n",
    "    if not is_valid(move) or not is_legal(move, player, board):\n",
    "        raise IllegalMoveError(player, move, copy)\n",
    "    return move\n",
    "\n",
    "def score(player, board):\n",
    "    \"\"\"Compute player's score (number of player's pieces minus opponent's).\"\"\"\n",
    "    mine, theirs = 0, 0\n",
    "    opp = opponent(player)\n",
    "    for sq in squares():\n",
    "        piece = board[sq]\n",
    "        if piece == player:\n",
    "            mine += 1\n",
    "        elif piece == opp:\n",
    "            theirs += 1\n",
    "    return mine - theirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"strategies\"></a>\n",
    "\n",
    "## Play Strategies\n",
    "\n",
    "### Random\n",
    "\n",
    "The easiest strategy to implement: simply pick a move at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_strategy(player, board):\n",
    "    \"\"\"A strategy that always chooses a random legal move.\"\"\"\n",
    "    return random.choice(legal_moves(player, board))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"localmax\"></a>\n",
    "\n",
    "### Local Maximization\n",
    "\n",
    "A more sophisticated strategy could look at every available move and evaluate them in some way. This consists of getting a list of legal moves, applying each one to a copy of the board, and choosing the move that results in the \"best\" board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizer(evaluate):\n",
    "    \"\"\"\n",
    "    Construct a strategy that chooses the best move by maximizing\n",
    "    evaluate(player, board) over all boards resulting from legal moves.\n",
    "    \"\"\"\n",
    "    def strategy(player, board):\n",
    "        def score_move(move):\n",
    "            return evaluate(player, make_move(move, player, board[:]))\n",
    "        return max(legal_moves(player, board), key=score_move)\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible evaluation function is `score`. A strategy constructed with `maximizer(score)` will always make the move that results in the largest immediate gain in pieces.\n",
    "\n",
    "A more advanced evaluation function might consider the relative worth of each square on the board and weight the score by the value of the pieces held by each player. Since corners and (most) edge squares are very valuable, we could weight those more heavily, and add negative weights to the squares that, if acquired, could lead to the opponent capturing the corners or edges.\n",
    "\n",
    "<img src='files/img/weighted.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUARE_WEIGHTS = [\n",
    "    0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "    0, 120, -20,  20,   5,   5,  20, -20, 120,   0,\n",
    "    0, -20, -40,  -5,  -5,  -5,  -5, -40, -20,   0,\n",
    "    0,  20,  -5,  15,   3,   3,  15,  -5,  20,   0,\n",
    "    0,   5,  -5,   3,   3,   3,   3,  -5,   5,   0,\n",
    "    0,   5,  -5,   3,   3,   3,   3,  -5,   5,   0,\n",
    "    0,  20,  -5,  15,   3,   3,  15,  -5,  20,   0,\n",
    "    0, -20, -40,  -5,  -5,  -5,  -5, -40, -20,   0,\n",
    "    0, 120, -20,  20,   5,   5,  20, -20, 120,   0,\n",
    "    0,   0,   0,   0,   0,   0,   0,   0,   0,   0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A strategy constructed as `maximizer(weighted_score)`, then will always return the move that results in the largest immediate \"weighted\" gain in pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_score(player, board):\n",
    "    \"\"\"\n",
    "    Compute the difference between the sum of the weights of player's\n",
    "    squares and the sum of the weights of opponent's squares.\n",
    "    \"\"\"\n",
    "    opp = opponent(player)\n",
    "    total = 0\n",
    "    for sq in squares():\n",
    "        if board[sq] == player:\n",
    "            total += SQUARE_WEIGHTS[sq]\n",
    "        elif board[sq] == opp:\n",
    "            total -= SQUARE_WEIGHTS[sq]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a greedy strategy however, and will result in a local optimum, as the accepted optimal strategy for Othello is to centralize and maximize mobility (the amount of legal moves available to you) by minimizing your pieces and restricting the opponent's mobility.\n",
    "\n",
    "<img src='files/img/greedy.png'>\n",
    "\n",
    "We're going to discuss this concept further in later section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"minimax\"></a>\n",
    "\n",
    "### Minimax search\n",
    "\n",
    "The maximizer strategies are very short-sighted, and a player who can consider the implications of a move several turns in advance could have a significant advantage. We can improve the strategy by searching ahead. Instead of choosing the move that leads immediately to the highest score, we can also consider the opponent's possible replies, our replies to those replies, and so on. By searching through several levels of moves, we can steer away from potential disaster and find good moves that were not immediately apparent.\n",
    "\n",
    "<img src='files/img/minimax.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax(player, board, depth, evaluate):\n",
    "    \"\"\"\n",
    "    Find the best legal move for player, searching to the specified depth.\n",
    "    Returns a tuple (min_score, move), where min_score is the guaranteed minimum\n",
    "    score achievable for player if the move is made.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We define the value of a board to be the opposite of its value to our\n",
    "    # opponent, computed by recursively applying `minimax` for our opponent.\n",
    "    def value(board):\n",
    "        return -minimax(opponent(player), board, depth-1, evaluate)[0]\n",
    "    \n",
    "    # When depth is zero, don't examine possible moves. Just determine the value\n",
    "    # of this board to the player.\n",
    "    if depth == 0:\n",
    "        return evaluate(player, board), None\n",
    "    \n",
    "    # We want to evaluate all the legal moves by considering their implications\n",
    "    # `depth` turns in advance. First, find all the legal moves.\n",
    "    moves = legal_moves(player, board)\n",
    "    \n",
    "    # If player has no legal moves, then either:\n",
    "    if not moves:\n",
    "        # the game is over, so the best achievable score is victory or defeat\n",
    "        if not any_legal_move(opponent(player), board):\n",
    "            return final_value(player, board), None\n",
    "        # or we have to pass this turn, so just find the value of this board.\n",
    "        return value(board), None\n",
    "    \n",
    "    # When there are multiple legal moves available, choose the best one by\n",
    "    # maximizing the value of the resulting boards.\n",
    "    return max((value(make_move(m, player, board[:])), m) for m in moves)\n",
    "\n",
    "# Values for endgame boards are big constants\n",
    "MAX_VALUE = float('inf')\n",
    "MIN_VALUE = float('-inf')\n",
    "\n",
    "def final_value(player, board):\n",
    "    \"\"\"The game is over. Find the value of this board to player.\"\"\"\n",
    "    diff = score(player, board)\n",
    "    if diff < 0:\n",
    "        return MIN_VALUE\n",
    "    elif diff > 0:\n",
    "        return MAX_VALUE\n",
    "    return diff\n",
    "\n",
    "def minimax_searcher(depth, evaluate):\n",
    "    \"\"\"\n",
    "    Construct a strategy that uses `minimax` with the specified leaf board\n",
    "    evaluation function.\n",
    "    \"\"\"\n",
    "    def strategy(player, board):\n",
    "        return minimax(player, board, depth, evaluate)[1]\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alphabeta\"></a>\n",
    "\n",
    "### Alpha-Beta Search\n",
    "\n",
    "Minimax is very effective, but it does too much work: it evaluates many search trees that should be ignored.\n",
    "\n",
    "Consider what happens when minimax is evaluating two moves, M1 and M2, on one level of a search tree. Suppose minimax determines that M1 can result in a score of S. While evaluating M2, if minimax finds a move in its subtree that could result in a better score than S, the algorithm should immediately quit evaluating M2: the opponent will force us to play M1 to avoid the higher score resulting from M2, so we shouldn't waste time determining just how much better M2 is than M1.\n",
    "\n",
    "<img src='files/img/alphabeta.png'>\n",
    "\n",
    "We need to keep track of two values:\n",
    "-  __alpha:__ the maximum score achievable by any of the moves we have encountered.\n",
    "-  __beta:__ the score that the opponent can keep us under by playing other moves.\n",
    "\n",
    "When the algorithm begins, alpha is the smallest value and beta is the largest value. During evaluation, if we find a move that causes `alpha >= beta`, then we can quit searching this subtree since the opponent can prevent us from playing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabeta(player, board, alpha, beta, depth, evaluate):\n",
    "    \"\"\"\n",
    "    Find the best legal move for player, searching to the specified depth.\n",
    "    Like minimax but uses the bounds alpha and beta to prune branches.\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        return evaluate(player, board), None\n",
    "    \n",
    "    def value(board, alpha, beta):\n",
    "        # Like in `minimax`, the value of a board is the opposite of its value\n",
    "        # to the opponent. We pass in `-beta` and `-alpha` as the alpha and\n",
    "        # beta values, respectively, for the opponent, since `alpha` represents\n",
    "        # the best score we know we can achieve and is therefore the worst score\n",
    "        # achievable by the opponent. Similary, `beta` is the worst score that\n",
    "        # our opponent can hold us to, so it is the best score that they can\n",
    "        # achieve.\n",
    "        return -alphabeta(opponent(player), board, -beta, -alpha, depth-1, evaluate)[0]\n",
    "    \n",
    "    moves = legal_moves(player, board)\n",
    "    if not moves:\n",
    "        if not any_legal_move(opponent(player), board):\n",
    "            return final_value(player, board), None\n",
    "        return value(board, alpha, beta), None\n",
    "    \n",
    "    best_move = moves[0]\n",
    "    for move in moves:\n",
    "        if alpha >= beta:\n",
    "            # If one of the legal moves leads to a better score than beta, then\n",
    "            # the opponent will avoid this branch, so we can quit looking.\n",
    "            break\n",
    "        val = value(make_move(move, player, board[:]), alpha, beta)\n",
    "        if val > alpha:\n",
    "            # If one of the moves leads to a better score than the current best\n",
    "            # achievable score, then replace it with this one.\n",
    "            alpha = val\n",
    "            best_move = move\n",
    "    return alpha, best_move\n",
    "\n",
    "def alphabeta_searcher(depth, evaluate):\n",
    "    def strategy(player, board):\n",
    "        return alphabeta(player, board, MIN_VALUE, MAX_VALUE, depth, evaluate)[1]\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alphabeta2\"></a>\n",
    "\n",
    "### Alpha-Beta v2.0 - Move Ordering\n",
    "\n",
    "The alpha-beta cutoffs work when we have established a good move and another move proves to be not as good. Thus, we will be able to make cutoffs earlier if we ensure that good moves are considered first. Our current algorithm loops through the list of `legal_moves`, but `legal_moves` makes no attempt to order the moves in any way. We will call this the _random-ordering_ strategy (even though the ordering is not random at all, square 11 is always considered first, then 12, etc.).\n",
    "\n",
    "One way to try to generate good moves first is to search highly weighted squares first. Since `legal_moves` considers squares in the order defined by `squares`, all we have to do is redefine the list `squares`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squares():\n",
    "    \"\"\"\n",
    "    List all the valid squares on the board\n",
    "    sorted from highest to lowest weight.\n",
    "    \"\"\"\n",
    "    return sorted([i for i in range(11, 89) if 1 <= (i % 10) <= 8], key=lambda sq: SQUARE_WEIGHTS[sq], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the corner squares will automatically be considered first, followed by the other highly weighted squares. We call this the _static-ordering_ strategy, because the ordering is not random, but it does not change depending on the situation.\n",
    "\n",
    "A more informed way to try to generate good moves first is to sort the moves according to the evaluation function. This means making more evaluations. Previously, only the boards at the leaves of the search tree were evaluated. Now we need to evaluate every board. In order to avoid evaluating a board more than once, we make up a structure called a `node`, which holds a board, the square that was taken to result in that board, and the evaluation value of that board.\n",
    "\n",
    "```\n",
    "node = [square, board, value]\n",
    "```\n",
    "\n",
    "The search is the same except that nodes are passed around instead of boards, and the nodes are sorted by their value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabeta2(player, node, alpha, beta, depth, evaluate):\n",
    "    \"\"\"\n",
    "    Alphabeta search, sorting moves by evaluation function\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        return node[2], node\n",
    "\n",
    "    def value(node, alpha, beta):\n",
    "        return -alphabeta2(opponent(player), negate_value(node), -beta, -alpha, depth-1, evaluate)[0]\n",
    "    \n",
    "    board = node[1]\n",
    "    nodes = legal_nodes(player, board, evaluate)\n",
    "    \n",
    "    if not nodes:\n",
    "        if not any_legal_move(opponent(player), board):\n",
    "            return final_value(player, board), None\n",
    "        return value(node, alpha, beta), None\n",
    "    \n",
    "    best_node = nodes[0]\n",
    "    for move in nodes:\n",
    "        if alpha >= beta:\n",
    "            # If one of the legal moves leads to a better score than beta, then\n",
    "            # the opponent will avoid this branch, so we can quit looking.\n",
    "            break\n",
    "        val = value(move, alpha, beta)\n",
    "        if val > alpha:\n",
    "            # If one of the moves leads to a better score than the current best\n",
    "            # achievable score, then replace it with this one.\n",
    "            alpha = val\n",
    "            best_node = move\n",
    "    return alpha, best_node\n",
    "\n",
    "def negate_value(node):\n",
    "    \"\"\"Set the value of a node to its negative.\"\"\"\n",
    "    node[2] = -node[2]\n",
    "    return node\n",
    "\n",
    "def legal_nodes(player, board, evaluate):\n",
    "    \"\"\"Return a list of legal moves, each one packed into a node.\"\"\"\n",
    "    def fn(move):\n",
    "        new_board = make_move(move, player, board[:])\n",
    "        return [move, new_board, evaluate(player, new_board)]\n",
    "    moves = legal_moves(player, board)\n",
    "    nodes = sorted([fn(move) for move in moves], key=lambda node: node[2], reverse=True)\n",
    "    return nodes\n",
    "    \n",
    "def alphabeta_searcher2(depth, evaluate):\n",
    "    \"\"\"Return a strategy that does Alphabeta search with sorted moves.\"\"\"\n",
    "    def strategy(player, board):\n",
    "        node = [None, board, evaluate(player, board)]\n",
    "        return alphabeta2(player, node, MIN_VALUE, MAX_VALUE, depth, evaluate)[1][0]\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alphabeta3\"></a>\n",
    "\n",
    "### Alpha-Beta v3.0 - Killer Instinct\n",
    "\n",
    "We saw before that the search routines look at tens of thousands of boards per move. Currently, each board position is created anew by `board[:]` and discarded soon thereafter. We could avoid generating all this garbage by reusing the same board at each depth. We'd still need to keep the board from the previous depth for use when the search backs up. Thus, a vector of boards is needed. In the following we assume that we will never search deeper than 40 depth. This is a safe assumption, as even the fastest Othello programs can only search about 15 depth before running out of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply_boards = [initial_board() for _ in range(40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we considered to possibility of searching moves in a different order, in an attempt to search the better moves first, thereby getting more alpha-beta pruning. In this section, we consider the _killer heuristic_, which states that a move that has proven to be a good one in one line of play is also likely to be a good one in another line of play. To use chess as perhaps a more familiar example, suppose I consider one move, and it leads to the opponent replying by capturing my queen. This is a killer move, one that I would like to avoid. Therefore, when I consider other possible moves, I want to immediately consider the possibility of the opponent making that queen-capturing move.\n",
    "\n",
    "The function `alphabeta3` adds the parameter `killer`, which is the best move found so far at the current level. After we determine the `legal_moves`, we use `put_first` to put the killer move first, if it is in fact a legal move. When it comes time to search the next level, we keep track of the best move in `killer2`. This requires keeping track of the value of the best move in `killer2_val`. Everything else is unchanged, except that we get a new board by recycling the `ply_boards` vector rather than by allocating fresh ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabeta3(player, board, alpha, beta, depth, evaluate, killer):\n",
    "    \"\"\"\n",
    "    Alphabeta search, putting killer move first.\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        return evaluate(player, board), None\n",
    "    \n",
    "    def value(board, alpha, beta, killer):\n",
    "        val, reply = alphabeta3(opponent(player), board, -beta, -alpha, depth-1, evaluate, killer)\n",
    "        return -val, reply\n",
    "    \n",
    "    moves = put_first(killer, legal_moves(player, board))\n",
    "    if not moves:\n",
    "        if not any_legal_move(opponent(player), board):\n",
    "            return final_value(player, board), None\n",
    "        return value(board, alpha, beta, None)[0], None\n",
    "    \n",
    "    best_move = moves[0]\n",
    "    new_board = ply_boards[depth]\n",
    "    killer2 = None\n",
    "    killer2_val = MAX_VALUE\n",
    "    for move in moves:\n",
    "        if alpha >= beta:\n",
    "            # If one of the legal moves leads to a better score than beta, then\n",
    "            # the opponent will avoid this branch, so we can quit looking.\n",
    "            break\n",
    "        val, reply = value(make_move(move, player, replace(new_board, board)), alpha, beta, killer2)\n",
    "        if val > alpha:\n",
    "            # If one of the moves leads to a better score than the current best\n",
    "            # achievable score, then replace it with this one.\n",
    "            alpha = val\n",
    "            best_move = move\n",
    "        if reply is not None and val < killer2_val:\n",
    "            # If one of the moves leads to a reply that is worse than our worst\n",
    "            # case scenario killer2, then replace it with this one.\n",
    "            killer2 = reply\n",
    "            killer2_val = val\n",
    "    return alpha, best_move\n",
    "\n",
    "def put_first(killer, moves):\n",
    "    \"\"\"\n",
    "    Move the killer move to the front of moves,\n",
    "    if the killer move is in fact a legal move.\n",
    "    \"\"\"\n",
    "    if killer in moves:\n",
    "        moves.insert(0, moves.pop(moves.index(killer)))\n",
    "    return moves\n",
    "\n",
    "def replace(seq1, seq2):\n",
    "    \"\"\"Copies one sequence into another\"\"\"\n",
    "    seq1[:] = seq2\n",
    "    return seq1\n",
    "    \n",
    "def alphabeta_searcher3(depth, evaluate):\n",
    "    \"\"\"Return a strategy that does Alphabeta search with killer moves\"\"\"\n",
    "    def strategy(player, board):\n",
    "        return alphabeta3(player, board, MIN_VALUE, MAX_VALUE, depth, evaluate, None)[1]\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"championship\"></a>\n",
    "## Championship Programs: Iago and Bill\n",
    "As mentioned in the introduction, the unpredictability of Othello makes it a difficult game for humans to master, and thus programs that search deeply can do comparatively well. In fact, in 1981 the reigning champion, Jonathan Cerf, proclaimed \"In my opinion the top programs ... are now equal (if not superior) to the best human players.\" In discussing Rosenbloom's Iago program (1982), Cerf went on to say \"I understand Paul Rosenbloom is interested in arranging a match against me. Unfortunately my schedule is very full, and I'm going to see that it remains that way for the foreseeable future.\"\n",
    "\n",
    "In 1989, another program, Bill (Lee and Mahajan 1990) beat the highest rated American Othello player, Brian Rose, by a score of 56-8. Bill's evaluation function is fast enough to search 6-8 depths under tournament conditions, yet it is so accurate that it beats its creator, Kai-Fu Lee, searching only 1 depth. (However, Lee is only a novice Othello player; his real interest is in speech recognition; see Waibel and Lee 1991.) There are other programs that also play at a high level, but they have not been written up in the AI literature as Iago and Bill have.\n",
    "\n",
    "In this section we present an evaluation function based on Iago's, although it also contains elements of Bill, and of an evaluation function written by Eric Wefald in 1989. The evaluation function makes use of two main features: _mobility_ and _edge stability_.\n",
    "\n",
    "<a id=\"mobility\"></a>\n",
    "### Mobility\n",
    "\n",
    "Both Iago and Bill make heavy use of the concept of mobility. Mobility is a measure of the ability to make moves; basically, the more moves one can make, the better. This is not quite true, because there is no advantage in being able to make bad moves, but it is a useful heuristic. We define _current mobility_ as the number of legal moves available to a player, and _potential mobility_ as the number of blank squares that are adjacent to opponent's pieces (also called _frontier discs_). These include the legal moves. A better measure of mobility would try to count only good moves. The following function computes both current and potential mobility for a player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj(square):\n",
    "    \"\"\"List all the neighbors of a square.\"\"\"\n",
    "    return [square + d for d in DIRECTIONS]\n",
    "\n",
    "def mobility(player, board):\n",
    "    \"\"\"\n",
    "    Current mobility is the number of legal moves.\n",
    "    Potential mobility is the number of blank squares\n",
    "    adjacent to an opponent that are not legal moves.\n",
    "    Returns current and potential mobility for player.\n",
    "    \"\"\"\n",
    "    opp = opponent(player)\n",
    "    current, potential = 0, 0\n",
    "    for sq in squares():\n",
    "        if board[sq] == EMPTY:\n",
    "            if sq in legal_moves(player, board):\n",
    "                current += 1\n",
    "            elif any(board[neighbor] == opp for neighbor in adj(sq)):\n",
    "                potential += 1\n",
    "    return current, (current + potential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"edgestability\"></a>\n",
    "### Edge Stability\n",
    "\n",
    "Success at Othello often hinges around edge play, and both Iago and Bill evaluate the edges carefully. Edge analysis is made easier by the fact that the edges are fairly independent of the interior of the board: once a piece is placed on the edge, no interior moves can flip it. This independence allows a simplifying assumption: to evaluate a position's edge strength, evaluate each of the four edges independently, without consideration of the interior of the board. The evaluation can be made more accurate by considering the X-squares to be part of the edge.\n",
    "\n",
    "Even evaluating a single edge is a time-consuming task, so Bill and Iago compile away the evaluation by building a table of all possible edge positions. An \"edge\" according to Bill is ten squares: the eight actual edge squares and the two X-squares. Since each square can be black, white, or empty, there are 310 or 59,049 possible edge positions-a large but manageable number.\n",
    "\n",
    "The value of each edge position is determined by a process of successive approximation. Just as in a minimax search, we will need a static edge evaluation function to determine the value of an edge position without search. This static edge evaluation function is applied to every possible edge position, and the results are stored in a 59,049 element vector. The static evaluation is just a weighted sum of the occupied squares, with different weights given depending on if the piece is stable or unstable.\n",
    "\n",
    "Each edge position's evaluation can be improved by a process of search. Iago uses a single depth search: given a position, consider all moves that could be made (including no move at all). Some moves will be clearly legal, because they flip pieces on the edge, but other moves will only be legal if there are pieces in the interior of the board to flip. Since we are only considering the edge, we don't know for sure if these moves are legal. They will be assigned probabilities of legality. The updated evaluation of a position is determined by the values and probabilities of each move. This is done by sorting the moves by value and then summing the product of the value times the probability that the move can be made. This process of iterative approximation is repeated five times for each position. At that point, Rosenbloom reports, the values have nearly converged.\n",
    "\n",
    "In effect, this extends the depth of the normal alpha-beta search by including an edge-only search in the evaluation function. Since each edge position with _n_ pieces is evaluated as a function of the positions with _n + 1_ pieces, the search is complete - it is an implicit 10-depth search.\n",
    "\n",
    "Calculating edge stability is a bit more complicated than the other features. The first step is to define a variable, `edge_table`, which will hold the evaluation of each edge position, and a constant, `edge_and_x_lists`, which is a list of the squares on each of the four edges. Each edge has ten squares because the X-squares are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of values for edge positions\n",
    "edge_table = [0 for _ in range(3**10)]\n",
    "\n",
    "# The four edges (with their X-squares)/\n",
    "edge_and_x_lists = [[22, 11, 12, 13, 14, 15, 16, 17, 18, 27],\n",
    "                    [72, 81, 82, 83, 84, 85, 86, 87, 88, 77],\n",
    "                    [22, 11, 21, 31, 41, 51, 61, 71, 81, 72],\n",
    "                    [27, 18, 28, 38, 48, 58, 68, 78, 88, 77]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each edge we can compute an index into the edge table by building a 10-digit base-3 number, where each digit is 1 if the corresponding edge square is occupied by the player, 2 if by the opponent, and 0 if empty. The function `edge_index` computes this, and `edge_stability` sums the values of the four edge indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index(player, board, squares):\n",
    "    \"\"\"The index counts 1 for player; 2 for opponent,\n",
    "    on each square -- summed as a base 3 number.\"\"\"\n",
    "    opp = opponent(player)\n",
    "    index = 0\n",
    "    for sq in squares:\n",
    "        index *= 3\n",
    "        if board[sq] == player:\n",
    "            index += 1\n",
    "        elif board[sq] == opp:\n",
    "            index += 2\n",
    "    return index\n",
    "\n",
    "def edge_stability(player, board):\n",
    "    \"\"\"Total edge evaluation for player\"\"\"\n",
    "    score = sum(edge_table[edge_index(player, board, edge)] for edge in edge_and_x_lists)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `edge_stability` is all we will need in Iago's evaluation function, but we still need to generate the edge table. Since this needs to be done only once, we don't have to worry about efficiency. In particular, rather than invent a new data structure to represent edges, we will continue to use complete boards, even though they will be mostly empty. The computations for the edge table will be made on the top edge, from the point of view of black, with black to play. But the same table can be used for white, or for one of the other edges, because of the way the edge index is computed.\n",
    "\n",
    "Each position in the table is first initialized to a static value computed by a kind of weighted-squares metric, but with different weights depending on if a piece is in danger of being captured. After that, each position is updated by considering the possible moves that can be made from the position, and the values of each of these moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_edge = edge_and_x_lists[0]\n",
    "\n",
    "def init_edge_table():\n",
    "    \"\"\"Initialize `edge_table`, starting from the empty board.\"\"\"\n",
    "    # Initialize the static values\n",
    "    for n_pieces in range(11):\n",
    "        def fn(board, index):\n",
    "            edge_table[index] = static_edge_stability(BLACK, board)\n",
    "        map_edge_n_pieces(fn, BLACK, initial_board(), n_pieces, top_edge, 0)\n",
    "    # Now iterate five times trying to improve\n",
    "    for _ in range(5):\n",
    "        for n_pieces in range(9, 0, -1):\n",
    "            def fn(board, index):\n",
    "                edge_table[index] = possible_edge_moves_value(BLACK, board, index)\n",
    "            map_edge_n_pieces(fn, BLACK, initial_board(), n_pieces, top_edge, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `map_edge_n_pieces` iterates through all edge positions with a total of `n` pieces (of either color), applying a function to each such position. It also keeps a running count of the edge index as it goes. The function should accept two arguments: the board and the index. Note that a single board can be used for all the positions because squares are reset after they are used. The function has three cases: if the number of squares remaining is less than `n`, then it will be impossible to place `n` pieces on those squares, so we give up. If there are no more squares then `n` must also be zero, so this is a valid position, and the function `fn` is called. Otherwise we first try leaving the current square blank, then try filling it with player's piece, and then with the opponent's piece, in each case calling `map_edge_n_pieces` recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_edge_n_pieces(fn, player, board, n, squares, index):\n",
    "    \"\"\"\n",
    "    Call fn on all edges with n pieces.\n",
    "    Index counts 1 for player, 2 for opponent\n",
    "    \"\"\"\n",
    "    if len(squares) < n:\n",
    "        return\n",
    "    elif not squares:\n",
    "        fn(board, index)\n",
    "    else:\n",
    "        index3 = index * 3\n",
    "        sq = squares[0]\n",
    "        map_edge_n_pieces(fn, player, board, n, squares[1:], index3)\n",
    "        if n > 0 and board[sq] == EMPTY:\n",
    "            board[sq] = player\n",
    "            map_edge_n_pieces(fn, player, board, n-1, squares[1:], index3+1)\n",
    "            board[sq] = opponent(player)\n",
    "            map_edge_n_pieces(fn, player, board, n-1, squares[1:], index3+2)\n",
    "            board[sq] = EMPTY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `possible_edge_moves_value` searches through all possible moves to determine an edge value that is more accurate than a static evaluation. It loops through every empty square on the edge, calling `possible_edge_move` to return a (_probability value_) pair. Since it is also possible for a player not to make any move at all on an edge, the pair (`1.0`, _current value_) is also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_edge_moves_value(player, board, index):\n",
    "    \"\"\"\n",
    "    Consider all possible edge moves.\n",
    "    Combine their values into a single number.\n",
    "    \"\"\"\n",
    "    x = [(1.0, edge_table[index])]\n",
    "    y = [possible_edge_move(player, board, sq) for sq in top_edge if board[sq] == EMPTY]\n",
    "    possibilities = x + y\n",
    "    return combine_edge_moves(possibilities, player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of each position is determined by making the move on the board, then looking up in the table the value of the resulting position for the opponent, and negating it (since we are interested in the value to us, not to our opponent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_edge_move(player, board, sq):\n",
    "    \"\"\"Return a (prob, val) pair for a possible edge move.\"\"\"\n",
    "    num_player = 1 if player == BLACK else 2\n",
    "    new_board = replace(ply_boards[num_player], board)\n",
    "    make_move(sq, player, new_board)\n",
    "    prob = edge_move_probability(player, board, sq)\n",
    "    val = -edge_table[edge_index(opponent(player), new_board, top_edge)]\n",
    "    return (prob, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible moves are combined with `combine_edge_moves`, which sorts the moves best-first. (Since `init_edge_table` started from black's perspective, black tries to maximize and white tries to minimize scores.) We then go down the moves, increasing the total value by the value of each move times the probability of the move, and decreasing the remaining probability by the probability of the move. Since there will always be at least one move (pass) with probability 1.0, this is guaranteed to converge. In the end, we round off the total value, so that we can do the run-time calculations with integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_edge_moves(possibilities, player):\n",
    "    \"\"\"Combine the best moves.\"\"\"\n",
    "    prob = 1.0\n",
    "    val = 0.0\n",
    "    fn = True if player == BLACK else False\n",
    "    for pair in sorted(possibilities, key=lambda x: x[1], reverse=fn):\n",
    "        if prob < 0.0:\n",
    "            break\n",
    "        val += prob * pair[0] * pair[1]\n",
    "        prob -= prob * pair[0]\n",
    "    return round(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to compute the probability that each possible edge move is legal. These probabilities should reflect things such as the fact that it is easy to capture a corner if the opponent is in the adjacent X-square, and very difficult otherwise. First we define some functions to recognize corner and X-squares and relate them to their neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_xsqs = [(11, 22), (18, 27), (81, 72), (88, 77)]\n",
    "\n",
    "def corner_p(sq):\n",
    "    \"\"\"Return the tuple which contains the corner square\"\"\"\n",
    "    return next((x for x in corner_xsqs if x[0] == sq), None)\n",
    "\n",
    "def x_square_p(sq):\n",
    "    \"\"\"Return the tuple which contains the x-square\"\"\"\n",
    "    return next((x for x in corner_xsqs if x[1] == sq), None)\n",
    "\n",
    "def x_square_for(corner):\n",
    "    \"\"\"Return the x-square for a corner square\"\"\"\n",
    "    tuple = [x for x in corner_xsqs if x[0] == corner]\n",
    "    return tuple[0][1] if tuple else None\n",
    "\n",
    "def corner_for(xsq):\n",
    "    \"\"\"Return the corner square for an x-square\"\"\"\n",
    "    tuple = [x for x in corner_xsqs if x[1] == xsq]\n",
    "    return tuple[0][0] if tuple else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we consider the probabilities. There are four cases. First, since we don't know anything about the interior of the board, we assume each player has a 50% chance of being able to play in an X-square. Second, if we can show that a move is legal (because it flips opponent pieces on the edge) then it has 100% probability. Third, for the corner squares, we assign a 90% chance if the opponent occupies the X-square, 10% if it is empty and only .1% if we occupy it. Otherwise, the probability is determined by the two neighboring squares: if a square is next to one or more opponents, it is more likely we can move there; if it is next to our pieces it is less likely. If it is legal for the opponent to move into the square, then the chances are cut in half (although we may still be able to move there, since we move first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_move_probability(player, board, square):\n",
    "    \"\"\"What's the probability that player can move to this square?\"\"\"\n",
    "    # X-squares\n",
    "    if x_square_p(square):\n",
    "        return 0.5\n",
    "    # Immediate capture\n",
    "    elif is_legal(square, player, board):\n",
    "        return 1.0\n",
    "    # Move to corner depends on X-square\n",
    "    elif corner_p(square):\n",
    "        x_square = x_square_for(square)\n",
    "        if board[x_square] == EMPTY:\n",
    "            return 0.1\n",
    "        elif board[x_square] == player:\n",
    "            return 0.001\n",
    "        else:\n",
    "            return 0.9\n",
    "    else:\n",
    "        val = [[.1, .4, .7],\n",
    "               [.05, .3, None],\n",
    "               [.01, None, None]]\n",
    "        x = count_edge_neighbors(player, board, square)\n",
    "        y = count_edge_neighbors(opponent(player), board, square)\n",
    "        if is_legal(square, opponent(player), board):\n",
    "            return val[x][y]/2\n",
    "        else:\n",
    "            return val[x][y]\n",
    "        \n",
    "def count_edge_neighbors(player, board, square):\n",
    "    \"\"\"Count the neighbors of this square occupied by player.\"\"\"\n",
    "    inc = [1, -1]\n",
    "    neighbors = [board[square+i] for i in inc]\n",
    "    return sum(1 for x in neighbors if x == player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we return to the problem of determining the static value of an edge position. This is computed by a weighted-squares metric, but the weights depend on the _stability_ of each piece. A piece is called stable if it cannot be captured, unstable if it is in immediate danger of being captured, and semistable otherwise. A table of weights follows for each edge square and stability. Note that corner squares are always stable, and X-squares we will call semistable if the adjacent corner is taken, and unstable otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                     stab  semi  un\n",
    "static_edge_table = [[None, 0, -2000],  # X\n",
    "                     [700, None, None], # corner\n",
    "                     [1200, 200, -25],  # C\n",
    "                     [1000, 200, 75],   # A\n",
    "                     [1000, 200, 50],   # B\n",
    "                     [1000, 200, 50],   # B\n",
    "                     [1000, 200, 75],   # A\n",
    "                     [1200, 200, -25],  # C\n",
    "                     [700, None, None], # corner\n",
    "                     [None, 0, -2000]]  # X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static evaluation then just sums each piece's value according to this table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_edge_stability(player, board):\n",
    "    \"\"\"Compute this edge's static stability.\"\"\"\n",
    "    score = 0\n",
    "    for i in range(len(top_edge)):\n",
    "        sq = top_edge[i]\n",
    "        if board[sq] == EMPTY:\n",
    "            score += 0\n",
    "        elif board[sq] == player:\n",
    "            score += static_edge_table[i][piece_stability(board, sq)]\n",
    "        else:\n",
    "            score -= static_edge_table[i][piece_stability(board, sq)]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of stability is fairly complex. It centers around finding the two \"pieces,\" `p1` and `p2`, which lay on either side of the piece in question and which are not of the same color as the piece. These \"pieces\" may be empty, or they may be off the board. A piece is unstable if one of the two is empty and the other is the opponent; it is semistable if there are opponents on both sides and at least one empty square to play on, or if it is surrounded by empty pieces. Finally, if either `p1` or `p2` is nil then the piece is stable, since it must be connected by a solid wall of pieces to the corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piece_stability(board, sq):\n",
    "    \"\"\"Evaluate whether a square is stable, unstable or semi-stable\"\"\"\n",
    "    stable, semi_stable, unstable = 0, 1, 2\n",
    "    if corner_p(sq):\n",
    "        return stable\n",
    "    elif x_square_p(sq):\n",
    "        if board[corner_for(sq)] == EMPTY:\n",
    "            return unstable\n",
    "        else:\n",
    "            return semi_stable\n",
    "    else:\n",
    "        player = board[sq]\n",
    "        opp = opponent(player)\n",
    "        p1 = next(p for p in board[sq:20] if p != player)\n",
    "        p2 = next(p for p in board[sq-1:9:-1] if p != player)\n",
    "        \n",
    "        # Unstable pieces can be captured immediately\n",
    "        # by playing in the empty square\n",
    "        if (p1 == EMPTY and p2 == opp) or (p2 == EMPTY and p1 == opp):\n",
    "            return unstable\n",
    "        \n",
    "        # Semi-stable pieces might be captured\n",
    "        elif p1 == opp and p2 == opp and next((p for p in board[11:19] if p == EMPTY), None):\n",
    "            return semi_stable\n",
    "        elif p1 == EMPTY and p2 == EMPTY:\n",
    "            return semi_stable\n",
    "        \n",
    "        # Stable pieces can never be captured\n",
    "        else:\n",
    "            return stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edge table can now be built by a call to `init_edge_table`. After the table is built once, it is a good idea to save it so that we won't need to repeat the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data into a file\n",
    "def save_data(filename, data):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(' '.join(str(x) for x in data))\n",
    "\n",
    "# Load data from file\n",
    "def load_data(filename):\n",
    "    with open(filename) as f:\n",
    "        dataset = [int(x) for x in next(f).split()]\n",
    "    return dataset\n",
    "\n",
    "init_edge_table()\n",
    "save_data('edge_table.txt', edge_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the Factors\n",
    "\n",
    "Now we have a measure of the three factors: current mobility, potential mobility and edge stability. All that remains is to find a good way to combine them into a single evaluation metric. The combination function used by Rosenbloom (1982) is a linear combination of the three factors, but each factor's coefficient is dependent on the move number. Rosenbloom's features are normalized to the range [-1000, 1000]; we normalize to the range [-1, 1] by doing a division after multiplying by the coefficient. That allows us to use integers for the coefficients. Since our three factors are not calculated in quite the same way as Rosenbloom's, it is not surprising that his coefficients are not the best for our program. The edge coefficient was doubled and the potential coefficient cut by a factor of five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Iago_eval(player, board):\n",
    "    \"\"\"\n",
    "    Combine edge stability, current mobility and\n",
    "    potential mobility to arrive at an evaluation.\n",
    "    \"\"\"\n",
    "    # The three factors are multiplied by coefficients\n",
    "    # that vary by move number\n",
    "    c_edg = 312000 + 6240 * move_number\n",
    "    if move_number < 25:\n",
    "        c_cur = 50000 + 2000 * move_number\n",
    "    else:\n",
    "        c_cur = 75000 + 1000 * move_number\n",
    "    c_pot = 20000\n",
    "    \n",
    "    p_cur, p_pot = mobility(player, board)\n",
    "    o_cur, o_pot = mobility(opponent(player), board)\n",
    "    \n",
    "    score1 = round(c_edg * edge_stability(player, board) / 32000)\n",
    "    score2 = round(c_cur * (p_cur - o_cur) / (p_cur + o_cur + 2))\n",
    "    score3 = round(c_pot * (p_pot - o_pot) / (p_pot + o_pot + 2))\n",
    "    \n",
    "    score = score1 + score2 + score3\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to code the `Iago` function. Given a search depth, `Iago` returns a strategy that will do alpha-beta search to that depth using the `Iago-eval` evaluation function. This version of Iago was able to defeat the modified weighted-squares strategy in 8 of 10 games at 3 depth, and 9 of 10 at 4 depth. On an Explorer II, 4-depth search takes about 20 seconds per move. At 5 depth, many moves take over a minute, so the program runs the risk of forfeiting. At 3 depth, the program takes only a few seconds per move, but it still was able to defeat the author in five straight games, by scores of 50-14, 64-0, 51-13, 49-15 and 36-28. Despite these successes, it is likely that the evaluation function could be improved greatly with a little tuning of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Iago(depth):\n",
    "    \"\"\"Use an approximation of Iago's evaluation function.\"\"\"\n",
    "    return alphabeta_searcher3(depth, Iago_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"other\"></a>\n",
    "## Other Techniques\n",
    "\n",
    "There are many other variations that can be tried to speed up the search and improve play. Unfortunately, choosing among the techniques is a bit of a black art. You will have to experiment to find the combination that is best for each domain and each evaluation function. Most of the following techniques were incorporated, or at least considered and rejected, in Bill.\n",
    "\n",
    "### Iterative Deepening\n",
    "\n",
    "We have seen that the average branching factor for Othello is about 10. This means that searching to depth _n_ + 1 takes roughly 10 times longer than search to depth _n_. Thus, we should be willing to go to a lot of overhead before we search one level deeper, to assure two things: that search will be done efficiently, and that we won't forfeit due to running out of time. A by-now familiar technique, iterative deepening, serves both these goals.\n",
    "\n",
    "Iterative deepening is used as follows. The strategy determines how much of the remaining time to allocate to each move. A simple strategy could allocate a constant amount of time for each move, and a more sophisticated strategy could allocate more time for moves at crucial points in the game. Once the time allocation is determined for a move, the strategy starts an iterative deepening alpha-beta search. There are two complications: First, the search at _n_ ply keeps track of the best moves, so that the search at _n_ + 1 ply will have better ordering information. In many cases it will be faster to do both the _n_ and _n_ + 1 ply searches with the ordering information than to do only the _n_ + 1 ply search without it. Second, we can monitor how much time has been taken searching each ply, and cut off the search when searching one more ply would exceed the allocated time limit. Thus, iterative-deepening search degrades gracefully as time limits are imposed. It will give a reasonable answer even with a short time allotment, and it will rarely exceed the allotted time.\n",
    "\n",
    "### Forward Pruning\n",
    "\n",
    "One way to cut the number of positions searched is to replace the legal move generator with a _plausible_ move generator: in other words, only consider good moves, and never even look at moves that seem clearly bad. This technique is called _forward pruning_. It has fallen on disfavor because of the difficulty in determining which moves are plausible. For most games, the factors that would go into a plausible move generator would be duplicated in the static evaluation function anyway, so forward pruning would require more effort without much gain. Worse, forward pruning could rule out a brilliant sacrifice - a move that looks bad initially but eventually leads to a gain.\n",
    "\n",
    "For some games, forward pruning is a necessity. The game of Go, for example, is played on a 19 by 19 board, so the first player has 361 legal moves, and a 6-ply search would involve over 2 quadrillion positions. However, many good Go programs can be viewed as not doing forward pruning but doing abstraction. There might be 30 empty squares in one portion of the board, and the program would treat a move to any of these squares equivalently.\n",
    "\n",
    "Bill uses forward pruning in a limited way to rule out certain moves adjacent to the corners. It does this not to save time but because the evaluation function might lead to such a move being selected, even though it is in fact a poor move. In other words, forward pruning is used to correct a bug in the evaluation function cheaply.\n",
    "\n",
    "### Nonspeculative Forward Pruning\n",
    "\n",
    "This technique makes use of the observation that there are limits in the amount the evaluation function can change from one position to the next. For example, if we are using the count difference as the evaluation function, then the most a move can change the evaluation is +37 (one for placing a piece in the corner, and six captures in each of the three directions). The smallest change is 0 (if the player is forced to pass). Thus, if there are 2 ply left in the search, and the backed-up value of position _A_ has been established as 28 points better than the static value of position _B_, then it is useless to expand position _B_. This assumes that we are evaluating every position, perhaps to do sorted ordering or iterative deepening. It also assumes that no position in the search tree is a final position, because then the evaluation could change by more than 37 points. In conclusion, it seems that nonspeculative forward pruning is not very useful for Othello, although it may play a role in other games.\n",
    "\n",
    "### Aspiration Search\n",
    "\n",
    "Alpha-beta search is initiated with the `alpha` and `beta` boundaries set to `MIN_VALUE` and `MAX_VALUE`, respectively. In other words, the search assumes nothing: the final position may be anything from a loss to a win. But suppose we are in a situation somewhere in the mid-game where we are winning by a small margin (say the static evaluation for the current position is 50). In most cases, a single move will not change the evaluation by very much. Therefore, if we invoked the alpha-beta search with a window defined by boundaries of, say, 0 and 100, two things can happen: if the actual backed-up evaluation for this position is in fact in the range 0 to 100, then the search will find it, and it will be found quickly, because the reduced window will cause more pruning. If the actual value is not in the range, then the value returned will reflect that, and we can search again using a larger window. This is called aspiration search, because we aspire to find a value within a given window. If the window is chosen well, then often we will succeed and will have saved some search time.\n",
    "\n",
    "Pearl (1984) suggests an alternative called zero-window search. At each level, the first possible move, which we'll call _m_, is searched using a reasonably wide window to determine its exact value, which we'll call _v_. Then the remaining possible moves are searched using _v_ as both the lower and upper bounds of the window. Thus, the result of the search will tell if each subsequent move is better or worse than _m_, but won't tell how much better or worse. There are three outcomes for zero-window search. If no move turns out to be better than _m_, then stick with _m_. If a single move is better, then use it. If several moves are better than _m_, then they have to be searched again using a wider window to determine which is best.\n",
    "\n",
    "There is always a trade-off between time spent searching and information gained. Zero-window search makes an attractive trade-off: we gain some search time by losing information about the value of the best move. We are still guaranteed of finding the best move, we just don't know its exact value.\n",
    "\n",
    "Bill's zero-window search takes only 63% of the time taken by full alpha-beta search. It is effective because Bill's move ordering techniques ensure that the first move is often best. With random move ordering, zero-window search would not be effective.\n",
    "\n",
    "### Think-Ahead\n",
    "\n",
    "A program that makes its move and then waits for the opponent's reply is wasting half the time available to it. A better use of time is to compute, or _think-ahead_ while the opponent is moving. Think-ahead is one factor that helps Bill defeat Iago. While many programs have done think-ahead by choosing the most likely move by the opponent and then starting an iterative-deepening search assuming that move, Bill's algorithm is somewhat more complex. It can consider more than one move by the opponent, depending on how much time is available.\n",
    "\n",
    "### Hashing and Opening Book Moves\n",
    "\n",
    "We have been treating the search space as a tree, but in general it is a directed acyclic graph (dag): there may be more than one way to reach a particular position, but there won't be any loops, because every move adds a new piece. This raises the question: should we treat the search space as a tree or a graph? By treating it as a graph we eliminate duplicate evaluations, but we have the overhead of storing all the previous positions, and of checking to see if a new position has been seen before. The decision must be based on the proportion of duplicate positions that are actually encountered in play. One compromise solution is to store in a hash table a partial encoding of each position, encoded as, say, a single integer (one word) instead of the seven or so words needed to represent a full board. Along with the encoding of each position, store the move to try first. Then, for each new position, look in the hash table, and if there is a hit, try the corresponding move first. The move may not even be legal, if there is an accidental hash collision, but there is a good chance that the move will be the right one, and the overhead is low.\n",
    "\n",
    "One place where it is clearly worthwil to store information about previous positions is in the opening game. Since there are fewer choices in the opening, it is a good idea to compile an opening \"book\" of moves and to play by it as long as possible, until the opponent makes a move that departs from the book. Book moves can be gleaned from the literature, although not very much has been written about Othello (as compared to openings in chess). However, there is a danger in following expert advice: the positions that an expert thinks are advantageous may not be the same as the positions from which our program can play well. It may be better to compile the book by playing the program against itself and determining which positions work out best.\n",
    "\n",
    "### The End Game\n",
    "\n",
    "It is also a good idea to try to save up time in the midgame and then make an all-out effort to search the complete game tree to completion as soon as feasible. Bill can search to completion from about 14 ply out. Once the search is done, of course, the most promising lines of play should be saved so that it won't be necessary to solve the game tree again.\n",
    "\n",
    "### Metareasoning\n",
    "\n",
    "If it weren't for the clock, Othello would be a trivial game: just search the complete game tree all the way to the end, and then choose the best move. The clock imposes a complication: we have to make all our moves before we run out of time. The algorithms we have seen so far manage the clock by allocating a certain amount of time to each move, such that the total time is guaranteed (or at least very likely) to be less than the allotted time. This is a very crude policy. A finer-grained way of managing time is to consider computation itself as a possible move. That is, at every tick of the clock, we need to decide if it is better to stop and play the best move we have computed so far or to continue and try to compute a better move. It will be better to compute more only in the case where we eventually choose a better move; it will be better to stop and play only in the case where we could otherwise forfeit due to time constraints, or be forced to make poor choices later in the game. An algorithm that includes computation as a possible move is called a metareasoning system, because it reasons about how much to reason.\n",
    "\n",
    "Russell and Wefald (1989) present an approach based on this view. In addition to an evaluation function, they assume a variance function, which gives an estimate of how much a given position's true value is likely to vary from its static value. At each step, their algorithm compares the value and variance of the best move computed so far and the second best move. If the best move is clearly better than the second best (taking variance into account), then there is no point computing any more. Also, if the top two moves have similar values but both have very low variance, then computing will not help much; we can just choose one of the two at random.\n",
    "\n",
    "For example, if the board is in a symmetric position, then there may be two symmetric moves that will have identical value. By searching each move's subtree more carefully, we soon arrive at a low variance for both moves, and then we can choose either one, without searching further. Of course, we could also add special-case code to check for symmetry, but the metareasoning approach will work for nonsymmetric cases as well as symmetric ones. If there is a situation where two moves both lead to a clear win, it won't waste time choosing between them.\n",
    "\n",
    "The only situation where it makes sense to continue computing is when there are two moves with high variance, so that it is uncertain if the true value of one exceeds the other. The metareasoning algorithm is predicated on devoting time to just this case.\n",
    "\n",
    "### Learning\n",
    "\n",
    "From the earliest days of computer game playing, it was realized that a championship program would need to learn to improve itself. Samuel (1959) describes a program that plays checkers and learns to improve its evaluation function. The evaluation function is a linear combination of features, such as the number of pieces for each player, the number of kings, the number of possible forks, and so on. Learning is done by a hill-climbing search procedure: change one of the coefficients for one of the features at random, and then see if the changed evaluation function is better than the original one.\n",
    "\n",
    "Without some guidance, this hill-climbing search would be very slow. First, the space is very large - Samuel used 38 different features, and although he restricted the coefficients to be a power of two between 0 and 20, that still leaves 2138 possible evaluation functions. Second, the obvious way of determining the relative worth of two evaluation functions - playing a series of games between them and seeing which wins more often - is quite time-consuming.\n",
    "\n",
    "Fortunately, there is a faster way of evaluating an evaluation function. We can apply the evaluation function to a position and compare this static value with the backed-up value determined by an alpha-beta search. If the evaluation function is accurate, the static value should correlate well with the backed-up value. If it does not correlate well, the evaluation function should be changed in such a way that it does. This approach still requires the trial-and-error of hill-climbing, but it will converge much faster if we can gain information from every position, rather than just from every game.\n",
    "\n",
    "In the past few years there has been increased interest in learning by a process of guided search. _Neural nets_ are one example of this. They have been discussed elsewhere. Another example is _genetic learning_ algorithms. These algorithms start with several candidate solutions. In our case, each candidate would consist of a set of coefficients for an evaluation function. On each generation, the genetic algorithm sees how well each candidate does. The worst candidates are eliminated, and the best ones \"mate\" and \"reproduce\" - two candidates are combined in some way to yield a new one. If the new offspring has inherited both its parents' good points, then it will prosper; if it has inherited both its parents' bad points, then it will quickly die out. Either way, the idea is that natural selection will eventually yield a high-quality solution. To increase the chances of this, it is a good idea to allow for mutations: random changes in the genetic makeup of one of the candidates.\n",
    "\n",
    "<a id=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "\n",
    "The strategies we've discussed are very general and are applicable to a broad range of strategy games, such as Chess, Checkers, and Go. More advanced strategies for Othello exist that apply various gameplay heuristics; some of these are discussed in \"Paradigms of Artificial Intelligence Programming\" by Peter Norvig.\n",
    "\n",
    "See Wikipedia for more details on [minimax](http://en.wikipedia.org/wiki/Minimax) and [alpha-beta](http://en.wikipedia.org/wiki/Alpha-beta_pruning) search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
