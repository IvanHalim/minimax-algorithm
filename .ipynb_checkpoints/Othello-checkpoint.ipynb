{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Othello\n",
    "\n",
    "Othello is a turn-based two-player strategy board game. The players take turns placing pieces, one player white and the other player black, on an 8x8 board in such a way that captures some of the opponent's pieces, with the goal of finishing the game with more pieces of their color on the board.\n",
    "\n",
    "Every move must capture one or more of the opponent's pieces. To capture, player A places a piece adjacent to one of player B's pieces so that there is a straight line (horizontal, vertical or diagonal) of adjacent pieces that begins with one of player A's pieces, continues with one or more of player B's pieces, and ends with one of player A's pieces.\n",
    "\n",
    "For example, if Black places a piece on square (5, 1), he will capture all of Black's pieces (5, 1) and (5, 6):\n",
    "\n",
    "<img src='files/img/capture.png'>\n",
    "\n",
    "For more information about the game (which is also known as Reversi) including detailed rules, see the entry on [Wikipedia](https://en.wikipedia.org/wiki/Reversi). Additionally, this implementation doesn't take into account some tournament-style Othello details, such as game time limits and a different indexing scheme.\n",
    "\n",
    "We will implement representations for the board and pieces and the mechanics of playing a game. We will then explore several game-playing strategies. There is a simple command-line program provided for playing against the computer or comparing two strategies.\n",
    "\n",
    "Written by [Daniel Connelly](http://dhconnelly.com). This implementation follows chapter 18 of Peter Norvig's \"Paradigms of Artificial Intelligence\".\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Board representation](#board)\n",
    "2. [Playing the game](#playing)\n",
    "3. [Strategies](#strategies)\n",
    "   - [Random](#random)<br>\n",
    "   - [Local maximization](#localmax)<br>\n",
    "   - [Minimax search](#minimax)<br>\n",
    "   - [Alpha-beta search](#alphabeta)<br>\n",
    "4. [Conclusion](#conclusion)\n",
    "\n",
    "<a id=\"board\"></a>\n",
    "\n",
    "## Board Representation\n",
    "\n",
    "We represent the board as a 100-element list, which includes each square on the board as well as the outside edge. Each consecutive sublist of ten elements represents a single row, and each list element stores a piece. An initial board contains four pieces in the center:\n",
    "\n",
    "```\n",
    "     ? ? ? ? ? ? ? ? ? ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . o @ . . . ?\n",
    "     ? . . . @ o . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? . . . . . . . . ?\n",
    "     ? ? ? ? ? ? ? ? ? ?\n",
    "```\n",
    "\n",
    "This representation has two useful properties:\n",
    "1. Square (m, n) can be accessed as `board[mn]`. This avoids the need to write functions that convert between square locations and list indexes.\n",
    "2. Operations involving bounds checking are slightly simpler.\n",
    "\n",
    "The outside edge is marked `?`, empty squares are `.`, black is `@`, and white is `o`. The black and white pieces represent the two players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY, BLACK, WHITE, OUTER = '.', '@', 'o', '?'\n",
    "PIECES = (EMPTY, BLACK, WHITE, OUTER)\n",
    "PLAYERS = {BLACK: 'Black', WHITE: 'White'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refer to neighbor squares we can add a direction to a square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP, DOWN, LEFT, RIGHT = -10, 10, -1, 1\n",
    "UP_RIGHT, DOWN_RIGHT, DOWN_LEFT, UP_LEFT = -9, 11, 9, -11\n",
    "DIRECTIONS = (UP, UP_RIGHT, RIGHT, DOWN_RIGHT, DOWN, DOWN_LEFT, LEFT, UP_LEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 2 3 4 5 6 7 8\n",
      "1 . . . . . . . .\n",
      "2 . . . . . . . .\n",
      "3 . . . . . . . .\n",
      "4 . . . o @ . . .\n",
      "5 . . . @ o . . .\n",
      "6 . . . . . . . .\n",
      "7 . . . . . . . .\n",
      "8 . . . . . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def squares():\n",
    "    \"\"\"List all the valid squares on the board\"\"\"\n",
    "    return [i for i in range(11, 89) if 1 <= (i % 10) <= 8]\n",
    "\n",
    "def initial_board():\n",
    "    \"\"\"Create a new board with the initial black and white positions filled\"\"\"\n",
    "    board = [OUTER] * 100\n",
    "    for i in squares():\n",
    "        board[i] = EMPTY\n",
    "    # The middle four squares should hold the initial piece positions.\n",
    "    board[44], board[45] = WHITE, BLACK\n",
    "    board[54], board[55] = BLACK, WHITE\n",
    "    return board\n",
    "\n",
    "def print_board(board):\n",
    "    \"\"\"Get a string representation of the board.\"\"\"\n",
    "    rep = ''\n",
    "    rep += '  {0}\\n'.format(' '.join(map(str, range(1, 9))))\n",
    "    for row in range(1, 9):\n",
    "        begin, end = 10*row + 1, 10*row + 9\n",
    "        rep += '{0} {1}\\n'.format(row, ' '.join(board[begin:end]))\n",
    "    return rep\n",
    "\n",
    "print(print_board(initial_board()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"playing\"></a>\n",
    "\n",
    "## Playing the Game\n",
    "\n",
    "We need functions to get moves from players, check to make sure that the moves are legal, apply the moves to the board, and detect when the game is over.\n",
    "\n",
    "### Checking Moves\n",
    "\n",
    "A move must be both valid and legal: it must refer to a real square, and it must form a bracket with another piece of the same color with pieces of the opposite color in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(move):\n",
    "    \"\"\"Is move a square on the board?\"\"\"\n",
    "    return isinstance(move, int) and move in squares()\n",
    "\n",
    "def opponent(player):\n",
    "    \"\"\"Get player's opponent piece.\"\"\"\n",
    "    return BLACK if player is WHITE else WHITE\n",
    "\n",
    "def find_bracket(square, player, board, direction):\n",
    "    \"\"\"\n",
    "    Find a square that forms a bracket with `square` for `player` in the given\n",
    "    `direction`. Returns None if no such squares exists.\n",
    "    \"\"\"\n",
    "    bracket = square + direction\n",
    "    if board[bracket] == player:\n",
    "        return None\n",
    "    opp = opponent(player)\n",
    "    while board[bracket] == opp:\n",
    "        bracket += direction\n",
    "    return None if board[bracket] in (OUTER, EMPTY) else bracket\n",
    "\n",
    "def is_legal(move, player, board):\n",
    "    \"\"\"Is this a legal move for the player?\"\"\"\n",
    "    hasbracket = lambda direction: find_bracket(move, player, board, direction)\n",
    "    return board[move] == EMPTY and any(map(hasbracket, DIRECTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Moves\n",
    "\n",
    "When the player makes a move, we need to update the board and flip all the bracketed pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_move(move, player, board):\n",
    "    \"\"\"Update the board to reflect the move by the specified player.\"\"\"\n",
    "    board[move] = player\n",
    "    for d in DIRECTIONS:\n",
    "        make_flips(move, player, board, d)\n",
    "    return board\n",
    "\n",
    "def make_flips(move, player, board, direction):\n",
    "    \"\"\"Flip pieces in the given direction as a result of the move by player.\"\"\"\n",
    "    bracket = find_bracket(move, player, board, direction)\n",
    "    if not bracket:\n",
    "        return\n",
    "    square = move + direction\n",
    "    while square != bracket:\n",
    "        board[square] = player\n",
    "        square += direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IllegalMoveError(Exception):\n",
    "    def __init__(self, player, move, board):\n",
    "        self.player = player\n",
    "        self.move = move\n",
    "        self.board = board\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '{0} cannot move to square {1}'.format(PLAYERS[self.player], self.move)\n",
    "    \n",
    "def legal_moves(player, board):\n",
    "    \"\"\"Get a list of all legal moves for player.\"\"\"\n",
    "    return [sq for sq in squares() if is_legal(sq, player, board)]\n",
    "\n",
    "def any_legal_move(player, board):\n",
    "    \"\"\"Can player make any moves?\"\"\"\n",
    "    return any(is_legal(sq, player, board) for sq in squares())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Each round consists of:\n",
    "-  Get a move from the current player.\n",
    "-  Apply it to the board.\n",
    "-  Switch players. If the game is over, get the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(black_strategy, white_strategy):\n",
    "    \"\"\"Play a game of Othello and return the final board and score.\"\"\"\n",
    "    board = initial_board()\n",
    "    player = BLACK\n",
    "    strategy = lambda who: black_strategy if who == BLACK else white_strategy\n",
    "    while player is not None:\n",
    "        move = get_move(strategy(player), player, board)\n",
    "        make_move(move, player, board)\n",
    "        player = next_player(board, player)\n",
    "    return board, score(BLACK, board)\n",
    "\n",
    "def next_player(board, prev_player):\n",
    "    \"\"\"Which player should move next? Returns None if no legal moves exist.\"\"\"\n",
    "    opp = opponent(prev_player)\n",
    "    if any_legal_move(opp, board):\n",
    "        return opp\n",
    "    elif any_legal_move(prev_player, board):\n",
    "        return prev_player\n",
    "    return None\n",
    "\n",
    "def get_move(strategy, player, board):\n",
    "    \"\"\"Call strategy(player, board) to get a move.\"\"\"\n",
    "    copy = list(board) # copy the board to prevent cheating\n",
    "    move = strategy(player, copy)\n",
    "    if not is_valid(move) or not is_legal(move, player, board):\n",
    "        raise IllegalMoveError(player, move, copy)\n",
    "    return move\n",
    "\n",
    "def get_score(player, board):\n",
    "    \"\"\"Compute player's score (number of player's pieces minus opponent's).\"\"\"\n",
    "    mine, theirs = 0, 0\n",
    "    opp = opponent(player)\n",
    "    for sq in squares():\n",
    "        piece = board[sq]\n",
    "        if piece == player:\n",
    "            mine += 1\n",
    "        elif piece == opp:\n",
    "            theirs += 1\n",
    "    return mine - theirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"strategies\"></a>\n",
    "\n",
    "## Play Strategies\n",
    "\n",
    "### Random\n",
    "\n",
    "The easiest strategy to implement: simply pick a move at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_strategy(player, board):\n",
    "    \"\"\"A strategy that always chooses a random legal move.\"\"\"\n",
    "    return random.choice(legal_moves(player, board))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"localmax\"></a>\n",
    "\n",
    "### Local Maximization\n",
    "\n",
    "A more sophisticated strategy could look at every available move and evaluate them in some way. This consists of getting a list of legal moves, applying each one to a copy of the board, and choosing the move that results in the \"best\" board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizer(evaluate):\n",
    "    \"\"\"\n",
    "    Construct a strategy that chooses the best move by maximizing\n",
    "    evaluate(player, board) over all boards resulting from legal moves.\n",
    "    \"\"\"\n",
    "    def strategy(player, board):\n",
    "        def score_move(move):\n",
    "            return evaluate(player, make_move(move, player, list(board)))\n",
    "        return max(legal_moves(player, board), key=score_move)\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible evaluation function is `score`. A strategy constructed with `maximizer(score)` will always make the move that results in the largest immediate gain in pieces.\n",
    "\n",
    "A more advanced evaluation function might consider the relative worth of each square on the board and weight the score by the value of the pieces held by each player. Since corners and (most) edge squares are very valuable, we could weight those more heavily, and add negative weights to the squares that, if acquired, could lead to the opponent capturing the corners or edges.\n",
    "\n",
    "<img src='files/img/weighted.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUARE_WEIGHTS = [\n",
    "    0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "    0, 120, -20,  20,   5,   5,  20, -20, 120,   0,\n",
    "    0, -20, -40,  -5,  -5,  -5,  -5, -40, -20,   0,\n",
    "    0,  20,  -5,  15,   3,   3,  15,  -5,  20,   0,\n",
    "    0,   5,  -5,   3,   3,   3,   3,  -5,   5,   0,\n",
    "    0,   5,  -5,   3,   3,   3,   3,  -5,   5,   0,\n",
    "    0,  20,  -5,  15,   3,   3,  15,  -5,  20,   0,\n",
    "    0, -20, -40,  -5,  -5,  -5,  -5, -40, -20,   0,\n",
    "    0, 120, -20,  20,   5,   5,  20, -20, 120,   0,\n",
    "    0,   0,   0,   0,   0,   0,   0,   0,   0,   0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A strategy constructed as `maximizer(weighted_score)`, then will always return the move that results in the largest immediate \"weighted\" gain in pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_score(player, board):\n",
    "    \"\"\"\n",
    "    Compute the difference between the sum of the weights of player's\n",
    "    squares and the sum of the weights of opponent's squares.\n",
    "    \"\"\"\n",
    "    opp = opponent(player)\n",
    "    total = 0\n",
    "    for sq in squares():\n",
    "        if board[sq] == player:\n",
    "            total += SQUARE_WEIGHTS[sq]\n",
    "        elif board[sq] == opp:\n",
    "            total -= SQUARE_WEIGHTS[sq]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a greedy strategy however, and will result in a local optimum, as the accepted optimal strategy for Othello is to centralize and maximize mobility (the amount of legal moves available to you) by minimizing your pieces and restricting the opponent's mobility.\n",
    "\n",
    "<img src='files/img/greedy.png'>\n",
    "\n",
    "We could construct an evaluation function `mobility`. A strategy constructed with `maximizer(mobility)` will always make the move that results in the largest immediate gain in mobility.\n",
    "\n",
    "We define _current mobility_ as the number of legal moves available to a player, and _potential mobility_ as the number of blank squares that are adjacent to opponent's pieces (called _frontier discs_). These include the legal moves. A better measure of mobility would try to count only good moves. The following function computes both current and potential mobility for a player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj(square):\n",
    "    \"\"\"List all the neighbors of a square.\"\"\"\n",
    "    return [square + d for d in DIRECTIONS]\n",
    "\n",
    "def mobility(player, board):\n",
    "    \"\"\"\n",
    "    Current mobility is the number of legal moves.\n",
    "    Potential mobility is the number of blank squares\n",
    "    adjacent to an opponent that are not legal moves.\n",
    "    Returns current and potential mobility for player.\n",
    "    \"\"\"\n",
    "    opp = opponent(player)\n",
    "    current, potential = 0, 0\n",
    "    for sq in squares():\n",
    "        if board[sq] == EMPTY:\n",
    "            if sq in legal_moves(player, board):\n",
    "                current += 1\n",
    "            elif any(board[neighbor] == opp for neighbor in adj[sq]):\n",
    "                potential += 1\n",
    "    return current, (current + potential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"minimax\"></a>\n",
    "\n",
    "### Minimax search\n",
    "\n",
    "The maximizer strategies are very short-sighted, and a player who can consider the implications of a move several turns in advance could have a significant advantage. We can improve the strategy by searching ahead. Instead of choosing the move that leads immediately to the highest score, we can also consider the opponent's possible replies, our replies to those replies, and so on. By searching through several levels of moves, we can steer away from potential disaster and find good moves that were not immediately apparent.\n",
    "\n",
    "<img src='files/img/minimax.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax(player, board, depth, evaluate):\n",
    "    \"\"\"\n",
    "    Find the best legal move for player, searching to the specified depth.\n",
    "    Returns a tuple (min_score, move), where min_score is the guaranteed minimum\n",
    "    score achievable for player if the move is made.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We define the value of a board to be the opposite of its value to our\n",
    "    # opponent, computed by recursively applying `minimax` for our opponent.\n",
    "    def value(board):\n",
    "        return -minimax(opponent(player), board, depth-1, evaluate)[0]\n",
    "    \n",
    "    # When depth is zero, don't examine possible moves. Just determine the value\n",
    "    # of this board to the player.\n",
    "    if depth == 0:\n",
    "        return evaluate(player, board), None\n",
    "    \n",
    "    # We want to evaluate all the legal moves by considering their implications\n",
    "    # `depth` turns in advance. First, find all the legal moves.\n",
    "    moves = legal_moves(player, board)\n",
    "    \n",
    "    # If player has no legal moves, then either:\n",
    "    if not moves:\n",
    "        # the game is over, so the best achievable score is victory or defeat\n",
    "        if not any_legal_move(opponent(player), board):\n",
    "            return final_value(player, board), None\n",
    "        # or we have to pass this turn, so just find the value of this board.\n",
    "        return value(board), None\n",
    "    \n",
    "    # When there are multiple legal moves available, choose the best one by\n",
    "    # maximizing the value of the resulting boards.\n",
    "    return max((value(make_move(m, player, list(board))), m) for m in moves)\n",
    "\n",
    "# Values for endgame boards are big constants\n",
    "MAX_VALUE = float('inf')\n",
    "MIN_VALUE = float('-inf')\n",
    "\n",
    "def final_value(player, board):\n",
    "    \"\"\"The game is over. Find the value of this board to player.\"\"\"\n",
    "    diff = score(player, board)\n",
    "    if diff < 0:\n",
    "        return MIN_VALUE\n",
    "    elif diff > 0:\n",
    "        return MAX_VALUE\n",
    "    return diff\n",
    "\n",
    "def minimax_searcher(depth, evaluate):\n",
    "    \"\"\"\n",
    "    Construct a strategy that uses `minimax` with the specified leaf board\n",
    "    evaluation function.\n",
    "    \"\"\"\n",
    "    def strategy(player, board):\n",
    "        return minimax(player, board, depth, evaluate)[1]\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alphabeta\"></a>\n",
    "\n",
    "### Alpha-Beta Search\n",
    "\n",
    "Minimax is very effective, but it does too much work: it evaluates many search trees that should be ignored.\n",
    "\n",
    "Consider what happens when minimax is evaluating two moves, M1 and M2, on one level of a search tree. Suppose minimax determines that M1 can result in a score of S. While evaluating M2, if minimax finds a move in its subtree that could result in a better score than S, the algorithm should immediately quit evaluating M2: the opponent will force us to play M1 to avoid the higher score resulting from M2, so we shouldn't waste time determining just how much better M2 is than M1.\n",
    "\n",
    "<img src='files/img/alphabeta.png'>\n",
    "\n",
    "We need to keep track of two values:\n",
    "-  __alpha:__ the maximum score achievable by any of the moves we have encountered.\n",
    "-  __beta:__ the score that the opponent can keep us under by playing other moves.\n",
    "\n",
    "When the algorithm begins, alpha is the smallest value and beta is the largest value. During evaluation, if we find a move that causes `alpha >= beta`, then we can quit searching this subtree since the opponent can prevent us from playing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabeta(player, board, alpha, beta, depth, evaluate):\n",
    "    \"\"\"\n",
    "    Find the best legal move for player, searching to the specified depth.\n",
    "    Like minimax but uses the bounds alpha and beta to prune branches.\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        return evaluate(player, board), None\n",
    "    \n",
    "    def value(board, alpha, beta):\n",
    "        # Like in `minimax`, the value of a board is the opposite of its value\n",
    "        # to the opponent. We pass in `-beta` and `-alpha` as the alpha and\n",
    "        # beta values, respectively, for the opponent, since `alpha` represents\n",
    "        # the best score we know we can achieve and is therefore the worst score\n",
    "        # achievable by the opponent. Similary, `beta` is the worst score that\n",
    "        # our opponent can hold us to, so it is the best score that they can\n",
    "        # achieve.\n",
    "        return -alphabeta(opponent(player), board, -beta, -alpha, depth-1, evaluate)[0]\n",
    "    \n",
    "    moves = legal_moves(player, board)\n",
    "    if not moves:\n",
    "        if not any_legal_move(opponent(player), board):\n",
    "            return final_value(player, board), None\n",
    "        return value(board, alpha, beta), None\n",
    "    \n",
    "    best_move = moves[0]\n",
    "    for move in moves:\n",
    "        if alpha >= beta:\n",
    "            # If one of the legal moves leads to a better score than beta, then\n",
    "            # the opponent will avoid this branch, so we can quit looking.\n",
    "            break\n",
    "        val = value(make_move(move, player, list(board)), alpha, beta)\n",
    "        if val > alpha:\n",
    "            # If one of the moves leads to a better score than the current best\n",
    "            # achievable score, then replace it with this one.\n",
    "            alpha = val\n",
    "            best_move = move\n",
    "    return alpha, best_move\n",
    "\n",
    "def alphabeta_searcher(depth, evaluate):\n",
    "    def strategy(player, board):\n",
    "        return alphabeta(player, board, MIN_VALUE, MAX_VALUE, depth, evaluate)[1]\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Championship Programs: Iago and Bill\n",
    "As mentioned in the introduction, the unpredictability of Othello makes it a difficult game for humans to master, and thus programs that search deeply can do comparatively well. In fact, in 1981 the reigning champion, Jonathan Cerf, proclaimed \"In my opinion the top programs ... are now equal (if not superior) to the best human players.\" In discussing Rosenbloom's Iago program (1982), Cerf went on to say \"I understand Paul Rosenbloom is interested in arranging a match against me. Unfortunately my schedule is very full, and I'm going to see that it remains that way for the foreseeable future.\"\n",
    "\n",
    "In 1989, another program, Bill (Lee and Mahajan 1990) beat the highest rated American Othello player, Brian Rose, by a score of 56-8. Bill's evaluation function is fast enough to search 6-8 depths under tournament conditions, yet it is so accurate that it beats its creator, Kai-Fu Lee, searching only 1 depth. (However, Lee is only a novice Othello player; his real interest is in speech recognition; see Waibel and Lee 1991.) There are other programs that also play at a high level, but they have not been written up in the AI literature as Iago and Bill have.\n",
    "\n",
    "In this section we present an evaluation function based on Iago's, although it also contains elements of Bill, and of an evaluation function written by Eric Wefald in 1989. The evaluation function makes use of two main features: _mobility_ and _edge stability_.\n",
    "\n",
    "# Edge Stability\n",
    "\n",
    "Success at Othello often hinges around edge play, and both Iago and Bill evaluate the edges carefully. Edge analysis is made easier by the fact that the edges are fairly independent of the interior of the board: once a piece is placed on the edge, no interior moves can flip it. This independence allows a simplifying assumption: to evaluate a position's edge strength, evaluate each of the four edges independently, without consideration of the interior of the board. The evaluation can be made more accurate by considering the X-squares to be part of the edge.\n",
    "\n",
    "Even evaluating a single edge is a time-consuming task, so Bill and Iago compile away the evaluation by building a table of all possible edge positions. An \"edge\" according to Bill is ten squares: the eight actual edge squares and the two X-squares. Since each square can be black, white, or empty, there are 310 or 59,049 possible edge positions-a large but manageable number.\n",
    "\n",
    "The value of each edge position is determined by a process of successive approximation. Just as in a minimax search, we will need a static edge evaluation function to determine the value of an edge position without search. This static edge evaluation function is applied to every possible edge position, and the results are stored in a 59,049 element vector. The static evaluation is just a weighted sum of the occupied squares, with different weights given depending on if the piece is stable or unstable.\n",
    "\n",
    "Each edge position's evaluation can be improved by a process of search. Iago uses a single depth search: given a position, consider all moves that could be made (including no move at all). Some moves will be clearly legal, because they flip pieces on the edge, but other moves will only be legal if there are pieces in the interior of the board to flip. Since we are only considering the edge, we don't know for sure if these moves are legal. They will be assigned probabilities of legality. The updated evaluation of a position is determined by the values and probabilities of each move. This is done by sorting the moves by value and then summing the product of the value times the probability that the move can be made. This process of iterative approximation is repeated five times for each position. At that point, Rosenbloom reports, the values have nearly converged.\n",
    "\n",
    "In effect, this extends the depth of the normal alpha-beta search by including an edge-only search in the evaluation function. Since each edge position with _n_ pieces is evaluated as a function of the positions with _n + 1_ pieces, the search is complete - it is an implicit 10-depth search.\n",
    "\n",
    "Calculating edge stability is a bit more complicated than the other features. The first step is to define a variable, `edge_table`, which will hold the evaluation of each edge position, and a constant, `edge_and_x_lists`, which is a list of the squares on each of the four edges. Each edge has ten squares because the X-squares are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of values for edge positions\n",
    "edge_table = [0 for _ in range(3**10)]\n",
    "\n",
    "# The four edges (with their X-squares)/\n",
    "edge_and_x_lists = [[22, 11, 12, 13, 14, 15, 16, 17, 18, 27],\n",
    "                    [72, 81, 82, 83, 84, 85, 86, 87, 88, 77],\n",
    "                    [22, 11, 21, 31, 41, 51, 61, 71, 81, 72],\n",
    "                    [27, 18, 28, 38, 48, 58, 68, 78, 88, 77]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each edge we can compute an index into the edge table by building a 10-digit base-3 number, where each digit is 1 if the corresponding edge square is occupied by the player, 2 if by the opponent, and 0 if empty. The function `edge_index` computes this, and `edge_stability` sums the values of the four edge indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index(player, board, squares):\n",
    "    \"\"\"The index counts 1 for player; 2 for opponent,\n",
    "    on each square -- summed as a base 3 number.\"\"\"\n",
    "    opp = opponent(player)\n",
    "    index = 0\n",
    "    for sq in squares:\n",
    "        index *= 3\n",
    "        if board[sq] == player:\n",
    "            index += 1\n",
    "        elif board[sq] == opp:\n",
    "            index += 2\n",
    "    return index\n",
    "\n",
    "def edge_stability(player, board):\n",
    "    \"\"\"Total edge evaluation for player\"\"\"\n",
    "    score = sum(edge_table[edge_index(player, board, edge)] for edge in edge_and_x_lists)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `edge_stability` is all we will need in Iago's evaluation function, but we still need to generate the edge table. Since this needs to be done only once, we don't have to worry about efficiency. In particular, rather than invent a new data structure to represent edges, we will continue to use complete boards, even though they will be mostly empty. The computations for the edge table will be made on the top edge, from the point of view of black, with black to play. But the same table can be used for white, or for one of the other edges, because of the way the edge index is computed.\n",
    "\n",
    "Each position in the table is first initialized to a static value computed by a kind of weighted-squares metric, but with different weights depending on if a piece is in danger of being captured. After that, each position is updated by considering the possible moves that can be made from the position, and the values of each of these moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_edge = edge_and_x_lists[0]\n",
    "\n",
    "def init_edge_table():\n",
    "    \"\"\"Initialize `edge_table`, starting from the empty board.\"\"\"\n",
    "    # Initialize the static values\n",
    "    for n_pieces in range(11):\n",
    "        def fn(board, index):\n",
    "            edge_table[index] = static_edge_stability(BLACK, board)\n",
    "        map_edge_n_pieces(fn, BLACK, initial_board(), n_pieces, top_edge, 0)\n",
    "    # Now iterate five times trying to improve\n",
    "    for _ in range(5):\n",
    "        for n_pieces in range(9, 0, -1):\n",
    "            def fn(board, index):\n",
    "                edge_table[index] = possible_edge_moves_value(BLACK, board, index)\n",
    "            map_edge_n_pieces(fn, BLACK, initial_board(), n_pieces, top_edge, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `map_edge_n_pieces` iterates through all edge positions with a total of `n` pieces (of either color), applying a function to each such position. It also keeps a running count of the edge index as it goes. The function should accept two arguments: the board and the index. Note that a single board can be used for all the positions because squares are reset after they are used. The function has three cases: if the number of squares remaining is less than `n`, then it will be impossible to place `n` pieces on those squares, so we give up. If there are no more squares then `n` must also be zero, so this is a valid position, and the function `fn` is called. Otherwise we first try leaving the current square blank, then try filling it with player's piece, and then with the opponent's piece, in each case calling `map_edge_n_pieces` recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_edge_n_pieces(fn, player, board, n, squares, index):\n",
    "    \"\"\"\n",
    "    Call fn on all edges with n pieces.\n",
    "    Index counts 1 for player, 2 for opponent\n",
    "    \"\"\"\n",
    "    if len(squares) < n:\n",
    "        return\n",
    "    elif not squares:\n",
    "        fn(board, index)\n",
    "    else:\n",
    "        index3 = index * 3\n",
    "        sq = squares[0]\n",
    "        map_edge_n_pieces(fn, player, board, n, squares[1:], index3)\n",
    "        if n > 0 and board[sq] == EMPTY:\n",
    "            board[sq] = player\n",
    "            map_edge_n_pieces(fn, player, board, n-1, squares[1:], index3+1)\n",
    "            board[sq] = opponent(player)\n",
    "            map_edge_n_pieces(fn, player, board, n-1, squares[1:], index3+2)\n",
    "            board[sq] = EMPTY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `possible_edge_moves_value` searches through all possible moves to determine an edge value that is more accurate than a static evaluation. It loops through every empty square on the edge, calling `possible_edge_move` to return a (_probability value_) pair. Since it is also possible for a player not to make any move at all on an edge, the pair (`1.0`, _current value_) is also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_edge_moves_value(player, board, index):\n",
    "    \"\"\"\n",
    "    Consider all possible edge moves.\n",
    "    Combine their values into a single number.\n",
    "    \"\"\"\n",
    "    x = [(1.0, edge_table[index])]\n",
    "    y = [possible_edge_move(player, board, sq) for sq in top_edge if board[sq] == EMPTY]\n",
    "    possibilities = x + y\n",
    "    combine_edge_moves(possibilities, player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of each position is determined by making the move on the board, then looking up in the table the value of the resulting position for the opponent, and negating it (since we are interested in the value to us, not to our opponent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_edge_move(player, board, sq):\n",
    "    \"\"\"Return a (prob, val) pair for a possible edge move.\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible moves are combined with `combine_edge_moves`, which sorts the moves best-first. (Since `init_edge_table` started from black's perspective, black tries to maximize and white tries to minimize scores.) We then go down the moves, increasing the total value by the value of each move times the probability of the move, and decreasing the remaining probability by the probability of the move. Since there will always be at least one move (pass) with probability 1.0, this is guaranteed to converge. In the end, we round off the total value, so that we can do the run-time calculations with integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_edge_moves(possibilities, player):\n",
    "    \"\"\"Combine the best moves.\"\"\"\n",
    "    prob = 1.0\n",
    "    val = 0.0\n",
    "    fn = True if player == BLACK else False\n",
    "    for pair in sorted(possibilities, key=lambda x: x[1], reverse=fn):\n",
    "        while prob >= 0.0:\n",
    "            val += prob * pair[0] * pair[1]\n",
    "            prob -= prob * pair[0]\n",
    "    return val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
